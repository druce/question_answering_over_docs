{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://colab.research.google.com/drive/1uL1TdMbR4kqa0Ksrd_Of_jWSxWt1ia7o?usp=sharing#scrollTo=c48a272c-8e87-4740-9960-129d7d5943bb\n",
    "# https://betterprogramming.pub/llamaindex-deep-lake-for-financial-statement-analysis-954f2b789c8e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- OpenAI API key\n",
    "- Install modules\n",
    "- Get data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for colab https://colab.research.google.com/github/druce/question_answering_over_docs/blob/main/10kAnalysis.ipynb\n",
    "\n",
    "# if using colab\n",
    "import os\n",
    "OPENAI_API_KEY=\"<mykey>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "!pip install llama-index pytesseract pdf2image\n",
    "# default in Colab is wrong version\n",
    "!pip uninstall rich\n",
    "!pip install rich==13.0.1\n",
    "\n",
    "# get data\n",
    "!mkdir uber\n",
    "!mkdir tmp\n",
    "!wget https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1 -O tmp/UBER.zip\n",
    "!unzip tmp/UBER.zip -d tmp\n",
    "!mv tmp/UBER/*.html ./uber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import interact, widgets\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import download_loader, ServiceContext, StorageContext, load_index_from_storage, GPTVectorStoreIndex\n",
    "from llama_index import GPTListIndex, LLMPredictor\n",
    "from llama_index.composability import ComposableGraph\n",
    "\n",
    "from langchain import OpenAI\n",
    "\n",
    "# if using dotenv with .env and OPENAI_API_KEY=<mykey>\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index data\n",
    "- Extract plain text from HTML\n",
    "- Chunk the text\n",
    "- For each chunk, make a semantic embedding using OpenAI API\n",
    "- store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/drucev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/drucev/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:unstructured:Reading document from string ...\n",
      "Reading document from string ...\n",
      "INFO:unstructured:Reading document ...\n",
      "Reading document ...\n",
      "INFO:unstructured:Reading document from string ...\n",
      "Reading document from string ...\n",
      "INFO:unstructured:Reading document ...\n",
      "Reading document ...\n",
      "INFO:unstructured:Reading document from string ...\n",
      "Reading document from string ...\n",
      "INFO:unstructured:Reading document ...\n",
      "Reading document ...\n",
      "INFO:unstructured:Reading document from string ...\n",
      "Reading document from string ...\n",
      "INFO:unstructured:Reading document ...\n",
      "Reading document ...\n"
     ]
    }
   ],
   "source": [
    "# extract raw text from html\n",
    "# https://unstructured.io ; https://github.com/Unstructured-IO/unstructured\n",
    "UnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n",
    "\n",
    "loader = UnstructuredReader()\n",
    "doc_set = {}\n",
    "all_docs = []\n",
    "years = [2022, 2021, 2020, 2019]\n",
    "for year in years:\n",
    "    year_docs = loader.load_data(file=Path(f'./uber/UBER_{year}.html'), split_documents=False)\n",
    "    # insert year metadata into each year\n",
    "    for d in year_docs:\n",
    "        d.extra_info = {\"year\": year}\n",
    "    doc_set[year] = year_docs\n",
    "    all_docs.extend(year_docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_context talks to openai (or other llm)\n",
    "# https://gpt-index.readthedocs.io/en/latest/reference/service_context.html\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make one index for each year and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vector indices + global vector index\n",
    "# NOTE: don't run this cell if the indices are already loaded! \n",
    "# generates many calls to openai to compute embedding vectors\n",
    "# https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemo.html\n",
    "\n",
    "index_set = {}\n",
    "\n",
    "for year in years:    \n",
    "    print(datetime.now(), 'indexing', year)\n",
    "    index_id = \"index_%d\" % year\n",
    "    cur_index = GPTVectorStoreIndex.from_documents(doc_set[year],\n",
    "                                                   service_context=service_context)\n",
    "    index_set[year] = cur_index\n",
    "    cur_index.storage_context.persist(index_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-07 10:51:02.230996 loading 2022\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "2023-05-07 10:51:02.549792 loading 2021\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "2023-05-07 10:51:02.884436 loading 2020\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "2023-05-07 10:51:03.345436 loading 2019\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# # load previously created indexes\n",
    "# index_set = {}\n",
    "# for year in years:\n",
    "#     index_id = \"index_%d\" % year\n",
    "#     print(datetime.now(), 'loading', year)\n",
    "#     # load index\n",
    "#     cur_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=index_id))\n",
    "#     index_set[year] = cur_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a global index for all years and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this global index is a single vector store containing all documents\n",
    "# Only relevant for the section below: \"Can a single vector index answer questions across years?\"\n",
    "# this generates many calls so run once and then load from index_global directory\n",
    "\n",
    "global_index = GPTVectorStoreIndex.from_documents(all_docs,\n",
    "                                                  service_context=service_context)\n",
    "global_index.storage_context.persist(\"index_global\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# global_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"index_global\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform question answering\n",
    "\n",
    "- user submits a question\n",
    "- query engine \n",
    "    - gets an semantic embedding for the question via OpenAI API\n",
    "    - searches the index for chunks of the source corpus matching the question\n",
    "    - submits a chatGPT query to Open AI to the effect of, answer this question using the following text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the index for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openai:error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "INFO:openai:error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n",
      "error_code=invalid_api_key error_message='Incorrect API key provided: sk-wmcZo***************************************DFCn. You can find your API key at https://platform.openai.com/account/api-keys.' error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index_set[\u001b[38;5;241m2020\u001b[39m]\u001b[38;5;241m.\u001b[39mas_query_engine(retriever_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m                                                service_context\u001b[38;5;241m=\u001b[39mservice_context,                                     \n\u001b[1;32m      3\u001b[0m                                                similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                                verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                                               )\n\u001b[1;32m      6\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat were some of the biggest risk factors in 2020?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/indices/query/base.py:20\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     19\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/query_engine/retriever_query_engine.py:139\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    136\u001b[0m query_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(CBEventType\u001b[38;5;241m.\u001b[39mQUERY)\n\u001b[1;32m    138\u001b[0m retrieve_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE)\n\u001b[0;32m--> 139\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m    141\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE, payload\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: nodes}, event_id\u001b[38;5;241m=\u001b[39mretrieve_id\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    144\u001b[0m synth_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/indices/base_retriever.py:21\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     20\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_llm_predict\u001b[39m(_self: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m         f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/indices/vector_store/retrievers.py:62\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m         event_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     59\u001b[0m             CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 62\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_service_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_agg_embedding_from_queries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_strs\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m     67\u001b[0m             CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}, event_id\u001b[38;5;241m=\u001b[39mevent_id\n\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     70\u001b[0m query \u001b[38;5;241m=\u001b[39m VectorStoreQuery(\n\u001b[1;32m     71\u001b[0m     query_embedding\u001b[38;5;241m=\u001b[39mquery_bundle\u001b[38;5;241m.\u001b[39membedding,\n\u001b[1;32m     72\u001b[0m     similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_similarity_top_k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha,\n\u001b[1;32m     77\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/embeddings/base.py:83\u001b[0m, in \u001b[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     queries: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     80\u001b[0m     agg_fn: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, List[\u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_embedding(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m     84\u001b[0m     agg_fn \u001b[38;5;241m=\u001b[39m agg_fn \u001b[38;5;129;01mor\u001b[39;00m mean_agg\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/embeddings/base.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     queries: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     80\u001b[0m     agg_fn: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, List[\u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     81\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_query_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m     84\u001b[0m     agg_fn \u001b[38;5;241m=\u001b[39m agg_fn \u001b[38;5;129;01mor\u001b[39;00m mean_agg\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/embeddings/base.py:72\u001b[0m, in \u001b[0;36mBaseEmbedding.get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_query_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_query_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     query_tokens_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer(query))\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_tokens_used \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m query_tokens_count\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/llama_index/embeddings/openai.py:223\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode, model combination: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m     engine \u001b[38;5;241m=\u001b[39m _QUERY_MODE_MODEL_DICT[key]\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama20/lib/python3.8/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query_engine = index_set[2020].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query = \"What were some of the biggest risk factors in 2020?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 11 tokens\n",
      "> [retrieve] Total embedding token usage: 11 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1402 tokens\n",
      "> [get_response] Total LLM token usage: 1402 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Some of the significant acquisitions include the divestiture of our ATG business to Aurora, our Uber Elevate business to Joby, our Yandex.Taxi joint venture in Russia/CIS, our agreement to enter into a joint venture with SK Telecom Co., LTD., our acquisition of Careem, our purchase of a controlling interest in Cornershop, and our acquisition of Postmates.\n"
     ]
    }
   ],
   "source": [
    "query = \"What were some of the signifcant acquisitions?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the global index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n",
      "> [retrieve] Total embedding token usage: 12 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1601 tokens\n",
      "> [get_response] Total LLM token usage: 1601 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "2019:\n",
      "- Drivers being classified as employees, workers or quasi-employees instead of independent contractors.\n",
      "- Highly competitive mobility, delivery, and logistics industries with well-established and low-cost alternatives.\n",
      "- Security breaches exposing the company to liability under various laws and regulations across jurisdictions.\n",
      "\n",
      "2020:\n",
      "- Drivers being classified as employees, workers or quasi-employees instead of independent contractors.\n",
      "- Highly competitive mobility, delivery, and logistics industries with well-established and low-cost alternatives.\n",
      "- Security breaches exposing the company to liability under various laws and regulations across jurisdictions.\n",
      "- Inability to anticipate and prevent security techniques and attacks.\n",
      "- Potential liability not covered by insurance.\n",
      "\n",
      "2022:\n",
      "- Drivers being classified as employees, workers or quasi-employees instead of independent contractors.\n",
      "- Highly competitive mobility, delivery, and logistics industries with well-established and low-cost alternatives.\n",
      "- Security breaches exposing the company to liability under various laws and regulations across jurisdictions.\n",
      "- Inability to anticipate and prevent security techniques and attacks.\n",
      "- Potential liability not covered by insurance.\n",
      "- Lowering of fares or service fees to remain competitive in certain markets.\n"
     ]
    }
   ],
   "source": [
    "query_all = global_index.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                         service_context=service_context,   \n",
    "                                         similarity_top_k=3,\n",
    "#                                          response_mode=\"tree_summarize\",\n",
    "                                         verbose=True,\n",
    "                                    )\n",
    "risk_query_str = \"What are some of the biggest risk factors in each year?\"\n",
    "response = query_all.query(risk_query_str)\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a global index by composing the indexes for individual years into a ComposableGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary text for each doc\n",
    "summaries = {}\n",
    "for year in years:\n",
    "    summaries[year] = f\"UBER 10-k Filing for {year} fiscal year\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of output tokens\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=1024))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    GPTListIndex,\n",
    "    [index_set[y] for y in years],\n",
    "    [summaries[y] for y in years],\n",
    "    service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_engines = {\n",
    "    my_index.index_id: my_index.as_query_engine(\n",
    "        similarity_top_k=3,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    for my_index in [index_set[y] for y in years]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_query_str = (\n",
    "    \"Describe the current risk factors. If the year is provided in the information, \"\n",
    "    \"provide that as well. If the context contains risk factors for multiple years, \"\n",
    "    \"explicitly provide the following:\\n\"\n",
    "    \"- A description of the risk factors for each year\\n\"\n",
    "    \"- A summary of how these risk factors are changing across years\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query ComposableGraph and note that it queries individual engines and collates an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 60 tokens\n",
      "> [retrieve] Total embedding token usage: 60 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 634 tokens\n",
      "> [get_response] Total LLM token usage: 634 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 634 tokens\n",
      "> [get_response] Total LLM token usage: 634 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 709 tokens\n",
      "> [get_response] Total LLM token usage: 709 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 709 tokens\n",
      "> [get_response] Total LLM token usage: 709 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 634 tokens\n",
      "> [get_response] Total LLM token usage: 634 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 634 tokens\n",
      "> [get_response] Total LLM token usage: 634 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 793 tokens\n",
      "> [get_response] Total LLM token usage: 793 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 793 tokens\n",
      "> [get_response] Total LLM token usage: 793 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 989 tokens\n",
      "> [get_response] Total LLM token usage: 989 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "The current risk factors for 2021 include the impact of the COVID-19 pandemic on parts of our business, the potential for Drivers to be classified as employees, workers or quasi-employees instead of independent contractors, the highly competitive nature of the mobility, delivery, and logistics industries, and the need to lower fares or service fees and offer Driver incentives and consumer discounts and promotions in order to remain competitive in certain markets. We have also incurred significant losses since inception, including in the United States and other major markets. \n",
      "\n",
      "The risk factors for 2020 are outlined in Item 1A of the information provided. These risk factors include unresolved staff comments, properties, legal proceedings, and mine safety disclosures. \n",
      "\n",
      "The risk factors for December 31, 2019 include interest rate risk, investment risk, and foreign currency risk. Interest rate risk relates to the 2016 Term Loan Facility and 2018 Term Loan Facility, which are floating rate notes and are carried at amortized cost. Investment risk is managed through an investment policy objective that aims to preserve capital and meet liquidity requirements without significantly increasing risk. As of December 31, 2019, cash and cash equivalents including restricted cash and cash equivalents totaled $8.2 billion and marketable debt securities classified as short-term investments totaled $440 million. \n",
      "\n",
      "Compared to prior years, the risk factors have not changed significantly. The COVID-19 pandemic has had an adverse effect on parts of our business, and the mobility, delivery, and logistics industries remain highly competitive. We have also continued to incur significant losses since inception.\n"
     ]
    }
   ],
   "source": [
    "query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n",
    "response = query_engine.query(risk_query_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: e2fef981-7dee-45b5-b23f-062302b3aca5): \n",
      "The current risk factors for 2022 include: Drivers being classified as employees, workers or qua...\n",
      "\n",
      "> Source (Doc id: 8da08af0-8b6a-4dc6-a4a4-235f1490f21d): \n",
      "The year provided in the context is 2021. The current risk factors include the impa\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources()[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 60 tokens\n",
      "> [retrieve] Total embedding token usage: 60 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2159 tokens\n",
      "> [get_response] Total LLM token usage: 2159 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "\n",
      "Year 2020:\n",
      "The risk factors include: Drivers being classified as employees, workers or quasi-employees instead of independent contractors; the mobility, delivery, and logistics industries being highly competitive; the need to lower fares or service fees to remain competitive; and the potential for significant losses since inception.\n",
      "\n",
      "Year 2021:\n",
      "The risk factors include: the COVID-19 pandemic and the impact of actions to mitigate the pandemic adversely affecting parts of the business; Drivers being classified as employees, workers or quasi-employees instead of independent contractors; the mobility, delivery, and logistics industries being highly competitive; and the need to lower fares or service fees and offer significant Driver incentives and consumer discounts and promotions to remain competitive.\n",
      "\n",
      "Year 2022:\n",
      "The risk factors include: Drivers being classified as employees, workers or quasi-employees instead of independent contractors; the mobility, delivery, and logistics industries being highly competitive; and the need to lower fares or service fees to remain competitive.\n",
      "\n",
      "Summary:\n",
      "The risk factors across the years include Drivers being classified as employees, workers or quasi-employees instead of independent contractors, the mobility, delivery, and logistics industries being highly competitive, and the need to lower fares or service fees to remain competitive. In 2021, the risk factor of the COVID-19 pandemic and the impact of actions to mitigate the pandemic was added, as well as the need to offer significant Driver incentives and consumer discounts and promotions to remain competitive.\n"
     ]
    }
   ],
   "source": [
    "# compare with global index\n",
    "response = global_index.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                        service_context=service_context,   \n",
    "                                        similarity_top_k=12,\n",
    "                                        verbose=True,\n",
    "                                       ).query(risk_query_str)\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6d/3xz907yn5ylg43s2vlnnzptr0000gn/T/ipykernel_20178/2936446814.py:61: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  text.on_submit(callback)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d79edbebb0947e798af32421fd31f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Index:', index=3, options=(('2019', <llama_index.query_engine.retriever_query_engine.Ret…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f861239402e14dcb8e1fa0d7b6c75a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='String:', placeholder='Enter prompt')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit questions using a text widget and dropdown for which index to query\n",
    "# todo use textarea\n",
    "# default question to value of risk_query_string\n",
    "# Describe the current risk factors. If the year is provided in the information, provide that as well. If the context contains risk factors for multiple years, explicitly provide the following: A description of the risk factors for each year; A summary of how these risk factors are changing across years\"\n",
    "# add submit button\n",
    "\n",
    "query_2019 = index_set[2019].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_2020 = index_set[2020].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_2021 = index_set[2021].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_2022 = index_set[2022].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_all = global_index.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                         service_context=service_context,   \n",
    "                                         similarity_top_k=3,\n",
    "#                                          response_mode=\"tree_summarize\",\n",
    "                                         verbose=True,\n",
    "                                         )\n",
    "query_all_graph = graph.as_query_engine(custom_query_engines=custom_query_engines)\n",
    "\n",
    "text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter prompt',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dd_val = None\n",
    "\n",
    "dd = widgets.Dropdown(\n",
    "    options = [('2019', query_2019), \n",
    "                   ('2020', query_2020), \n",
    "                   ('2021', query_2021), \n",
    "                   ('2022', query_2022), \n",
    "                   ('All years', query_all),\n",
    "                   ('All years using ComposableGraph', query_all_graph)],\n",
    "    index=3,\n",
    "    description='Index:',\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    global dd_val\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        dd_val = change['new']\n",
    "\n",
    "dd.observe(on_change)\n",
    "\n",
    "def callback(wdgt):\n",
    "    query_engine = dd_val\n",
    "    query = wdgt.value\n",
    "    print(\"Thinking...\")\n",
    "    response = query_engine.query(query)\n",
    "    print(response)\n",
    "\n",
    "text.on_submit(callback)\n",
    "\n",
    "display(dd)\n",
    "display(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama20",
   "language": "python",
   "name": "llama20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
