{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://colab.research.google.com/drive/1uL1TdMbR4kqa0Ksrd_Of_jWSxWt1ia7o?usp=sharing#scrollTo=c48a272c-8e87-4740-9960-129d7d5943bb\n",
    "# https://betterprogramming.pub/llamaindex-deep-lake-for-financial-statement-analysis-954f2b789c8e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab https://colab.research.google.com/github/druce/question_answering_over_docs/blob/main/10kAnalysis.ipynb\n",
    "\n",
    "# # if using colab\n",
    "# import os\n",
    "# OPENAI_API_KEY=\"<mykey>\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# !pip install llama-index pytesseract pdf2image\n",
    "# !pip uninstall rich\n",
    "# !pip install rich==13.0.1\n",
    "\n",
    "# # get data\n",
    "# !mkdir uber\n",
    "# !mkdir tmp\n",
    "# !wget https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1 -O tmp/UBER.zip\n",
    "# !unzip tmp/UBER.zip -d tmp\n",
    "# !mv tmp/UBER/*.html ./uber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "from ipywidgets import interact, widgets\n",
    "from pathlib import Path\n",
    "import panel as pn  # GUI\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# from llama_index import download_loader, ServiceContext, StorageContext, load_index_from_storage, GPTVectorStoreIndex\n",
    "# from llama_index import GPTListIndex, LLMPredictor\n",
    "# from llama_index.composability import ComposableGraph\n",
    "\n",
    "import langchain\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# if using dotenv with .env and OPENAI_API_KEY=<mykey>\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# if using colab\n",
    "# OPENAI_API_KEY=\"<mykey>\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir uber\n",
    "# mkdir tmp\n",
    "# wget https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1 -O tmp/UBER.zip\n",
    "# unzip tmp/UBER.zip -d tmp\n",
    "# mv tmp/UBER/*.html ./uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSocked In Color.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm=OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "llm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:unstructured:Reading document from string ...\n",
      "Reading document from string ...\n",
      "Reading document from string ...\n",
      "INFO:unstructured:Reading document ...\n",
      "Reading document ...\n",
      "Reading document ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'|\\n|UNITED STATES|\\n|SECURITIES AND EXCHANGE COMMISSION|\\n|Washington, D.C. 20549|\\n|FORM 10-K|\\n|☒    ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract raw text from html\n",
    "# https://unstructured.io ; https://github.com/Unstructured-IO/unstructured\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./docx-unstructured.md\", mode='elements')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs[1].page_content[:400]\n",
    "# UnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n",
    "\n",
    "# loader = UnstructuredReader()\n",
    "# all_docs = []\n",
    "\n",
    "# docs = loader.load_data(file=Path(f'./10K.html'), split_documents=False)\n",
    "#     # insert year metadata into each year\n",
    "# for d in docs:\n",
    "#     d.extra_info = {\"year\": 2022, \"ticker\": 'MSFT', \"name\": \"Microsoft\"}\n",
    "#     all_docs.extend(docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an AI assistant that answers questions about financial documents.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "# create a chain to answer questions \n",
    "# qa = ConversationalRetrievalChain.from_llm(OpenAI(model='gpt-3.5-turbo-16k'), retriever, return_source_documents=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=ChatOpenAI(model='gpt-3.5-turbo-16k'),\n",
    "                                           retriever=retriever,\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The risk factors mentioned in the context are as follows:\n",
      "\n",
      "1. Foreign exchange rate risk: The company is exposed to economic risk from foreign exchange rates, which may impact their consolidated financial statements. They use derivative instruments to manage this risk.\n",
      "\n",
      "2. Interest rate risk: The securities held in the company's fixed-income portfolio are subject to different interest rate risks based on their maturities. They manage the average maturity of the portfolio to achieve desired economic returns.\n",
      "\n",
      "3. Credit risk: The company's fixed-income portfolio consists primarily of investment-grade securities. They manage credit exposures relative to broad-based indices and to facilitate portfolio diversification.\n",
      "\n",
      "4. Equity price risk: Securities held in the company's equity investments portfolio are subject to price risk.\n",
      "\n",
      "These risk factors are disclosed in the context in relation to the company's exposure to market risks.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"what were the risk factors?\"\n",
    "result = qa({\"question\": query, 'chat_history': chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft is a technology company that develops, manufactures, licenses, supports, and sells a wide range of software, services, and hardware products. It was founded in 1975 by Bill Gates and Paul Allen and is headquartered in Redmond, Washington. Microsoft's mission is to empower every person and every organization on the planet to achieve more. The company is known for its flagship products such as the Windows operating system, Microsoft Office suite, and the Xbox gaming console. Microsoft also offers cloud-based services through its Azure platform and provides business solutions through its Dynamics product line. Additionally, Microsoft is involved in research and development, focusing on areas such as artificial intelligence, cloud computing, and productivity tools.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"what is Microsoft?\"\n",
    "result = qa({\"question\": query, 'chat_history': chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append((query, result[\"answer\"]))\n",
    "query=\"Where is Microsoft located?\"\n",
    "result = qa({\"question\": query, 'chat_history': chat_history})\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on my database, the airspeed velocity of an unladen swallow varies depending on the species. For example, the African swallow has been recorded flying at speeds up to 43 miles per hour, while the European swallow has been observed flying at speeds up to 36 miles per hour. However, it is important to note that the airspeed velocity can also be affected by factors such as wind conditions and the weight of the bird.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "openai.api_key=os.environ['OPENAI_API_KEY']\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    memory=ConversationSummaryBufferMemory(\n",
    "        llm=ChatOpenAI(), max_token_limit=2048\n",
    "    ),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(conversation.predict(input=\"what is the airspeed velocity of an unladen swallow?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD NOT HAVE TO BE SET TWICE BUT OTHERWISE WE GET AN AUTHENTICATION ERROR\n",
    "import openai\n",
    "openai.api_key=os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_context talks to openai (or other llm)\n",
    "# https://gpt-index.readthedocs.io/en/latest/reference/service_context.html\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512,\n",
    "                                               llm=llm)\n",
    "                                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context.llm_predictor.llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vector indices + global vector index\n",
    "# NOTE: don't run this cell if the indices are already loaded! \n",
    "# generates many calls to openai to compute embedding vectors\n",
    "# https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/SimpleIndexDemo.html\n",
    "year = 2022\n",
    "fmt = 'html'\n",
    "print(datetime.now(), 'indexing')\n",
    "index_id = \"index_%s_%d\" % (fmt, year)\n",
    "cur_index = GPTVectorStoreIndex.from_documents(docs,\n",
    "                                               service_context=service_context)\n",
    "cur_index.storage_context.persist(index_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously created indexes\n",
    "year = 2022\n",
    "fmt = 'html'\n",
    "index_id = \"index_%s_%d\" % (fmt, year)\n",
    "cur_index.storage_context.persist(index_id)\n",
    "\n",
    "print(datetime.now(), 'loading', fmt, year)\n",
    "# load index\n",
    "cur_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=index_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "fmt = 'docx'\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(datetime.now(), 'loading')\n",
    "docs = loader.load_data(file=Path(f'./10K.{fmt}'), split_documents=False)\n",
    "    # insert year metadata into each year\n",
    "for d in docs:\n",
    "    d.extra_info = {\"year\": 2022, \"ticker\": 'MSFT', \"name\": \"Microsoft\"}\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "    \n",
    "print(datetime.now(), 'indexing')\n",
    "index_id = \"index_%s_%d\" % (fmt, year)\n",
    "cur_index = GPTVectorStoreIndex.from_documents(docs,\n",
    "                                               service_context=service_context)\n",
    "cur_index.storage_context.persist(index_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "fmt = 'pdf'\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(datetime.now(), 'loading')\n",
    "docs = loader.load_data(file=Path(f'./10K.{fmt}'), split_documents=False)\n",
    "    # insert year metadata into each year\n",
    "for d in docs:\n",
    "    d.extra_info = {\"year\": 2022, \"ticker\": 'MSFT', \"name\": \"Microsoft\"}\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "    \n",
    "print(datetime.now(), 'indexing')\n",
    "index_id = \"index_%s_%d\" % (fmt, year)\n",
    "cur_index = GPTVectorStoreIndex.from_documents(docs,\n",
    "                                               service_context=service_context)\n",
    "cur_index.storage_context.persist(index_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "fmt = 'md'\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "print(datetime.now(), 'loading')\n",
    "docs = loader.load_data(file=Path(f'./docx-unstructured.{fmt}'), split_documents=False)\n",
    "    # insert year metadata into each year\n",
    "for d in docs:\n",
    "    d.extra_info = {\"year\": 2022, \"ticker\": 'MSFT', \"name\": \"Microsoft\"}\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "    \n",
    "print(datetime.now(), 'indexing')\n",
    "index_id = \"index_%s_%d\" % (fmt, year)\n",
    "cur_index = GPTVectorStoreIndex.from_documents(docs,\n",
    "                                               service_context=service_context)\n",
    "cur_index.storage_context.persist(index_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this global index is a single vector store containing all documents\n",
    "# Only relevant for the section below: \"Can a single vector index answer questions across years?\"\n",
    "# this generates many calls so run once and then load from index_global directory\n",
    "\n",
    "# global_index = GPTVectorStoreIndex.from_documents(all_docs,\n",
    "#                                                   service_context=service_context)\n",
    "# global_index.storage_context.persist(\"index_global\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myindex = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"index_md_2022\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = myindex.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                       service_context=service_context,\n",
    "                                       similarity_top_k=3,\n",
    "                                       verbose=True,\n",
    "                                      )\n",
    "query = \"What were some of the biggest risk factors?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was goodwill?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all = global_index.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                         service_context=service_context,   \n",
    "                                         similarity_top_k=3,\n",
    "#                                          response_mode=\"tree_summarize\",\n",
    "                                         verbose=True,\n",
    "                                    )\n",
    "risk_query_str = \"What are some of the biggest risk factors in each year?\"\n",
    "response = query_all.query(risk_query_str)\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary text for each doc\n",
    "summaries = {}\n",
    "for year in years:\n",
    "    summaries[year] = f\"UBER 10-k Filing for {year} fiscal year\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of output tokens\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, max_tokens=512))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    GPTListIndex,\n",
    "    [index_set[y] for y in years],\n",
    "    [summaries[y] for y in years],\n",
    "    service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_engines = {\n",
    "    my_index.index_id: my_index.as_query_engine(\n",
    "        similarity_top_k=1,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    for my_index in [index_set[y] for y in years]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_query_str = (\n",
    "    \"Describe the current risk factors. If the year is provided in the information, \"\n",
    "    \"provide that as well. If the context contains risk factors for multiple years, \"\n",
    "    \"explicitly provide the following:\\n\"\n",
    "    \"- A description of the risk factors for each year\\n\"\n",
    "    \"- A summary of how these risk factors are changing across years\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n",
    "response = query_engine.query(risk_query_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.get_formatted_sources()[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tmp = index_set[2022].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              ).query(risk_query_str)\n",
    "print(str(response_tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = global_index.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                        service_context=service_context,   \n",
    "                                        similarity_top_k=4,\n",
    "                                        verbose=True,\n",
    "                                       ).query(risk_query_str)\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension()\n",
    "\n",
    "query_engine = query_all\n",
    "c = 0\n",
    "\n",
    "def pn_callback(_):\n",
    "    prompt = inp.value\n",
    "    inp.value = prompt\n",
    "    response_text = ''\n",
    "    source_text = ''\n",
    "    \n",
    "    if prompt:\n",
    "        response = query_engine.query(prompt)\n",
    "        response_text = response.response\n",
    "        source_text = response.get_formatted_sources()[:300]\n",
    "\n",
    "    panels = []\n",
    "    panels.append(\n",
    "        pn.Row('Question:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Response:', pn.pane.Markdown(response_text, width=600, styles={'background-color': '#F6F6F6'})))\n",
    "    panels.append(\n",
    "        pn.Row('Sources:', pn.pane.Markdown(source_text, width=600, styles={'background-color': '#F6F6F6'})))\n",
    "    \n",
    "    return pn.Column(*panels)\n",
    "\n",
    "inp = pn.widgets.TextAreaInput(height=100,\n",
    "                               width=600,\n",
    "                               value='',\n",
    "                               placeholder='Enter question here…',\n",
    "                              )\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "\n",
    "interactive_conversation = pn.bind(pn_callback, button_conversation)\n",
    "\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit questions using a text widget and dropdown for which index to query\n",
    "# todo use textarea\n",
    "# default question to value of risk_query_string\n",
    "# Describe the current risk factors. If the year is provided in the information, provide that as well. If the context contains risk factors for multiple years, explicitly provide the following: A description of the risk factors for each year; A summary of how these risk factors are changing across years\"\n",
    "# add submit button\n",
    "\n",
    "query_2019 = index_set[2019].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_2020 = index_set[2020].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_2021 = index_set[2021].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_2022 = index_set[2022].as_query_engine(retriever_mode=\"embedding\", \n",
    "                                               service_context=service_context,                                     \n",
    "                                               similarity_top_k=3,\n",
    "                                               verbose=True,\n",
    "                                              )\n",
    "query_all = global_index.as_query_engine(retriever_mode=\"embedding\", \n",
    "                                         service_context=service_context,   \n",
    "                                         similarity_top_k=3,\n",
    "#                                          response_mode=\"tree_summarize\",\n",
    "                                         verbose=True,\n",
    "                                         )\n",
    "query_all_graph = graph.as_query_engine(custom_query_engines=custom_query_engines)\n",
    "\n",
    "text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter prompt',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dd = widgets.Dropdown(\n",
    "    options = [('2019', query_2019), \n",
    "                   ('2020', query_2020), \n",
    "                   ('2021', query_2021), \n",
    "                   ('2022', query_2022), \n",
    "                   ('All years', query_all),\n",
    "                   ('All years using ComposableGraph', query_all_graph)],\n",
    "    index=3,\n",
    "    description='Index:',\n",
    ")\n",
    "\n",
    "def on_change(change):\n",
    "    global dd_val\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        dd_val = change['new']\n",
    "\n",
    "dd.observe(on_change)\n",
    "\n",
    "def callback(wdgt):\n",
    "    query_engine = dd_val\n",
    "    query = wdgt.value\n",
    "    print(\"Thinking...\")\n",
    "    response = query_engine.query(query)\n",
    "    print(response)\n",
    "\n",
    "text.on_submit(callback)\n",
    "\n",
    "display(dd)\n",
    "display(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(risk_query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mammoth\n",
    "with open(\"10K.docx\", \"rb\") as docx_file:\n",
    "    result = mammoth.convert_to_markdown(docx_file)\n",
    "with open(\"docx-mammoth.md\", \"w\") as markdown_file:\n",
    "    markdown_file.write(result.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdownify import markdownify as md\n",
    "with open(\"10K.html\") as html_file:\n",
    "    html_str = \"\".join(html_file.readlines())\n",
    "with open(\"html-markdownify.md\", \"w\") as markdown_file:\n",
    "    markdown_file.write(md(html_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(md(html_str)[:999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "text = textract.process(\"10K.docx\")\n",
    "with open(\"docx-textract.md\", \"wb\") as markdown_file:\n",
    "    markdown_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "elements = partition(\"10K.pdf\")\n",
    "elements_txt = [e.text for e in elements]\n",
    "with open(\"pdf-unstructured.md\", \"w\") as markdown_file:\n",
    "    markdown_file.write(\"|\\n|\".join(elements_txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition(\"10K.docx\")\n",
    "elements_txt = [e.text for e in elements]\n",
    "with open(\"docx-unstructured.md\", \"w\") as markdown_file:\n",
    "    markdown_file.write(\"|\\n|\".join(elements_txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[2002].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa4",
   "language": "python",
   "name": "qa4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
